\chapter{Introduction}
\label{chap:intro}

\section{The Client}
\label{intro:client}
This project will be developed with the intention of it
being utilised by a client, for the benefit of their operation.
The client is
CX Labs UK within Cisco Systems. CX Labs provides lab space for
use by business
units internal to the company. Most of the space is used for
the testing of
customer networks by SVS (Solution Validation Services). SVS
provides bespoke
testing services to customers wishing to use Ciscoâ€™s expertise
to test a range
of scenarios, from regression and firmware testing to full
upgrade and
migration plans.\newline
To match the customer's environment as close as
possible, a scaled-down
version of the customer's network is usually recreated
in the lab space managed
by CX Labs. CX Labs hold many devices that cover most
of the Cisco portfolio,
which allows for the recreation of most networks. Most
of this lab space is
hosted within the internal Cisco corporate network, which
requires any users to
be employees of Cisco to access testbeds. More and more
customers
however are requesting remote access to their testbeds so they can
perform their own testing. To facilitate this, a
fully isolated DMZ environment
is provided, which allows direct WAN
connectivity to a testbed, allowing for a
VPN tunnel to be established and
hence remote access granted to a testbed from
any location to any permitted
person.

\subsubsection{Current Infrastructure}
The current infrastructure to support the testbeds consists of four Nexus 9K
core switches, with \gls{fex}s to provide copper connectivity. Currently, the
N9Ks do not have any form of \gls{vpc} configured, and just use \gls{stp} for
redundancy. Top-of-rack connectivity is provided by Catalyst switches. Each
project has a dedicated VLAN to isolate communications between different
projects, and a virtual router and services stack is hosted on a vCenter
environment to provide internet and \gls{vpn} connectivity to the project. Each
project also has several terminal servers attached to its VLAN so that the
console ports of the testbed devices can be easily accessed in case of a device
problem.

\subsubsection{Current Project Pipeline}
When a new testbed build is
requested, the project topology is reviewed, which will indicate how much
rackspace will be required to accommodate the project. Suitable racks will then
be chosen to house the project based on cooling, power and space requirements.
A new VLAN for the project will then be manually created across all of the
associated networking equipment, including the core N9Ks, and the top-of-rack
switches that live in the selected racks. Terminal servers will also have to be
reconfigured so that they have the correct subnet configured and the correct
sub-interface configured. The next step is to provision a virtual CSR1000v
router hosted on vCenter that will provide \gls{nat} and internet access to the
newly created VLAN. A services stack will then be deployed to take care of
remote access \gls{vpn} and other associated services. Both of these steps
require the manual addition of a \gls{dpg} to the \gls{dvs} present in vCenter.
\section{The Problem}
\label{intro:problem}
Whilst the existing solution does
function, several problems frequently arise which reduce the operational
efficiency of the lab:

A large initial time investment is required when
onboarding a new project. This is due to the manual configuration of the
networking equipment, as well as the manual deployment of the virtual router
and services stack. This time could be better spent on other tasks, such as
preparing the physical rackspace for the racking and stacking of new equipment.

No configuration management system is in place, which results in configuration
drift over time. When projects are removed, the configuration is sometimes
inconsistently modified on devices. Both VLANs and \gls{dpg}s become
unsynchronised and often differ between the core switches themselves, resulting
in confusion when modifying configuration. In the past, this has led to
accidental removal and modification of configuration related to other projects,
which impacts customer availability and takes time away from the team that
could otherwise have been used to tackle other issues.

No centralised
management is utilised which results in device firmware being hard to update.
This is because each device has to be manually updated, which is a
time-consuming process. This also means that the devices are not all running
the same version of firmware, which can cause issues when troubleshooting. A
lack of management makes device failure also hard to detect, as there is no
centralised monitoring solution in place, so reports from the testing team or
customer are often the first indication that a problem has arisen.

\section{Aims and Objectives}
\label{intro:aims}

This project aims to provide
a solution that will automate the
configuration of the DMZ network
infrastructure, as well as the configuration of the associated project
infrastructure, such as the project router, services stack and terminal
servers. This will be achieved by providing a web-based dashboard that allows
the user to provision a new project, which will then automatically configure
the required infrastructure. The solution will revolve around recreating the
physical rackspace inside the web interface. The concept behind this is a
one-to-one mapping between the real-world rackspace and the virtual rackspace.
The virtual rack can then have the \gls{tor} and terminal server that resides
in the real-world rack associated with it. A project can then be allocated
virtual racks and have the associated infrastructure that is tied to a rack
automatically provisioned. The solution will also provide a view of the current
utilisation of the lab space by projects, as well as the current utilisation of
the lab space as a whole.

The solution will use Cisco \gls{aci} to provide
network connectivity. This removes the complexities that would otherwise be
associated with provisioning many network devices via \gls{ssh} or RESTCONF.
\gls{aci} will also make the network highly scalable, with the ability to add
additional leafs and \gls{fex}s to support any expansion of the rackspace.
VMWare vCenter will be used to host virtual machines associated with project
infrastructure, such as the project's virtual router and services stack.

This
report will detail the design and conception of a testbed that will be used to
test the automation platform against \gls{aci} and vCenter. The testbed design
can then be used to build a fully-fledged infrastructure to support testing in
the future.
\subsection{Deliverables}
\label{intro:aims:deliverables}

\begin{itemize}
    \item A web-based dashboard that allows the user to manage
          and provision projects
    \item User guide for the dashboard

    \item User guide for how to provision ACI, vCenter and other associated
          network infrastructure so that it will be compatible with the automation
          solution
    \item This report, detailing the design and implementation of the
          solution
\end{itemize}

\section{Limitations and Risks}
\label{intro:constraints}
As the testbed that will be used to test and develop the solution is in a remote datacenter, physical access to the testbed is limited. This may result in delays if a problem that requires physical remediation occurs. The testbed will be designed to be resilient, through the use of redundant power supplies and links where possible. If a failure does occur, then the project may run over time, and key milestones may have to be adjusted accordingly. With the testbed being hosted remotely, there is also a possiblity that access to the testbed may be revoked, which would result in the project being delayed until access is restored. External factors such as the availability of the testbed's internet uplink and the power supply may also have an impact on the delivery of the project within the required timescale.